<!DOCTYPE html><html lang="en" mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.7.2"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="分类与聚类模型整理" /><meta name="author" content="Tour" /><meta property="og:locale" content="en_US" /><meta name="description" content="分类模型整理" /><meta property="og:description" content="分类模型整理" /><link rel="canonical" href="https://taolics.github.io/posts/%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/" /><meta property="og:url" content="https://taolics.github.io/posts/%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/" /><meta property="og:site_name" content="Tour" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-09-04T14:07:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="分类与聚类模型整理" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Tour"},"description":"分类模型整理","url":"https://taolics.github.io/posts/%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/","@type":"BlogPosting","headline":"分类与聚类模型整理","dateModified":"2020-09-04T14:07:00+08:00","datePublished":"2020-09-04T14:07:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://taolics.github.io/posts/%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/"},"@context":"https://schema.org"}</script><title>分类与聚类模型整理 | Tour</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/post.css" as="style"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://raw.githubusercontent.com/taolics/taolics.github.io/gh-pages/assets/img/favicons/android-icon-96x96.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Tour</a></div><div class="site-subtitle font-italic">Enjoy your life!</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/tabs/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tabs/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/tabs/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/tabs/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/taolics" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://www.cnblogs.com/icodes8238/" aria-label="cnblogs" target="_blank" rel="noopener"> <i class="fab fa-pinterest-p"></i> </a> <a href="https://space.bilibili.com/358322653" aria-label="bili" target="_blank" rel="noopener"> <i class="fab fa-slideshare"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['taolics','foxmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>分类与聚类模型整理</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>分类与聚类模型整理</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Sep 4, 2020, 2:07 PM +0800" > Sep 4, 2020 <i class="unloaded">2020-09-04T14:07:00+08:00</i> </span> by <span class="author"> Tour </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3635 words">20 min</span></div></div><div class="post-content"><h4 id="分类模型整理">分类模型整理</h4><h5 id="1svm">1、SVM</h5><ul><li><p>SVM算法特点与介绍</p><ul><li><p>训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以 SVM 不太容易产生 overfitting。</p><p>SVM 训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。</p><p>一个 SVM 如果训练得出的支持向量个数比较少，那么SVM 训练出的模型比较容易被泛化。</p><li><p>优点： SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题。</p><li><p>缺点： SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到$O(n_2)$的复杂度，这对大规模的数据是吃不消的。</p></ul><li><p>SVM原理</p><p>支持向量机（Support Vector Machine, SVM）的基本模型是在特征空间上找到最佳的分离超平面使得训练集上正负样本间隔最大。SVM是用来解决二分类问题的有监督学习算法，在引入了核方法之后SVM也可以用来解决非线性问题。 一般SVM有下面三种：</p><ul><li>硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。<li>软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化学得一个线性支持向量机。<li>非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化学得一个非线性支持向量机。</ul><p>1、周志华. 机器学习 [D]. 清华大学出版社，2016.</p><p>2、https://blog.csdn.net/liugan528/article/details/79448379</p><p>3、https://zhuanlan.zhihu.com/p/31886934</p><li><p>SVM应用场景</p><p>SVM在小样本训练集上能够得到比其它算法好很多的结果。支持向量机之所以成为目前最常用，效果最好的分类器之一，在于其优秀的泛化能力，这是是因为其本身的优化目标是结构化风险最小，而不是经验风险最小。</p><li><p>SVM使用方法</p><ul><li><p>python</p><p>https://blog.csdn.net/u012679707/article/details/80511968</p><p>重要参数详解：https://blog.csdn.net/github_39261590/article/details/75009069</p><p>训练svm分类器 kernel=’linear’时，为线性核，C越大分类效果越好，但有可能会过拟合（defaul C=1） kernel=’rbf’时（default），为高斯核，gamma值越小，分类界面越连续；gamma值越大，分类界面越“散”，分类效果越好，但有可能会过拟合。 decision_function_shape=’ovr’时，为one v rest（一对多），即一个类别与其他类别进行划分， decision_function_shape=’ovo’时，为one v one（一对一），即将类别两两之间进行划分，用二分#类的方法模拟多分类的结果。</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>from sklearn import svm
classifier=svm.SVC(C=2,kernel='rbf',gamma=10,decision_function_shape='ovo') 
classifier.fit(train_data,train_label.ravel()) #ravel函数在降维时默认是行序优先
</pre></table></code></div></div><li><p>matlab中使用</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>SVMStruct = svmtrain(TrainData,Group,'Showplot',true);       % train
Group = svmclassify(SVMStruct,TestData,'Showplot',true);     % test
</pre></table></code></div></div></ul></ul><h5 id="2逻辑回归算法">2、逻辑回归算法</h5><ul><li><p>逻辑回归算法特点与介绍</p><p>逻辑回归（Logistic Regression<strong>）</strong>是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 如给的一封邮件，判断是不是垃圾邮件。逻辑回归一般是提供样本和已知模型求回归参数。</p><p>一般是解决二分类问题，一般判断某个样本事件所属分类的概率。</p><li><p>逻辑回归使用</p><ul><li><p>python</p><p>重要参数解释：https://blog.csdn.net/Monk_donot_know/article/details/90720366?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre>from sklearn.linear_model import LogisticRegression
    
#训练模型
lr=LogisticRegression(C=1e5)
lr.fit(X,Y)
    
#预测
Z=lr.predict(np.c_[xx.ravel(),yy.ravel()])
Z=Z.reshape(xx.shape)
  
</pre></table></code></div></div><li><p>matlab</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>a =glmfit(x,y,'binomial', 'link', 'logit');  //用逻辑回归来计算系数矩阵
logitFit = glmval(a,x, 'logit'); //用逻辑回归的结果预测测试集的结果
</pre></table></code></div></div></ul></ul><h5 id="3决策树算法">3、决策树算法</h5><ul><li><p>决策树算法特点与介绍</p><p>决策树(Decision Tree）是在已知各种情况发生概率的<a href="https://baike.baidu.com/item/基础/32794">基础</a>上，通过构成决策树来求取净现值的<a href="https://baike.baidu.com/item/期望/35704">期望</a>值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。Entropy = 系统的凌乱程度，使用算法<a href="https://baike.baidu.com/item/ID3">ID3</a>, <a href="https://baike.baidu.com/item/C4.5">C4.5</a>和C5.0生成树算法使用熵。这一度量是基于信息学理论中熵的概念。</p><p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p><li><p>决策树的使用方法：</p><ul><li><p>python</p><p>https://www.cnblogs.com/juanjiang/p/11003369.html</p><p>https://blog.csdn.net/xiaoyw71/article/details/80110658?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf</p><p>重要参数详解：https://www.cnblogs.com/juanjiang/p/11003369.html</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>from sklearn import tree
'''使用信息熵作为划分标准，对决策树进行训练 '''  
clf = tree.DecisionTreeClassifier(criterion='entropy')  
print(clf)     
clf.fit(x_train, y_train)  
answer = clf.predict(x_train) 
</pre></table></code></div></div><li><p>matlab</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre>    
ctree = ClassificationTree.fit(P_train,T_train);
%%得到最小的叶子尺寸，再进行决策树生成
OptimalTree = fitctree(X,Y,'minleaf',40);
%%查看决策树
view(ctree);
view(ctree,'mode','graph');
%% 剪枝
[~,~,~,bestlevel] = cvLoss(ctree,'subtrees','all','treesize','min')
cptree = prune(ctree,'Level',bestlevel);
view(cptree,'mode','graph')
</pre></table></code></div></div></ul><li><p>决策树应用场景：</p><p>1、应用决策树决策方法必须具备以下条件：</p><p>（1）具有决策者期望达到的明确目标</p><p>（2）存在决策者可以选择的两个以上的可行的备选方案</p><p>（3）存在决策者无法控制的两个以上不确定因素</p><p>（4）不同方案在不同因素下的收益或损失可以计算出来</p><p>（5）决策者可以估计不确定因素发生的概率</p><p>2、从以上介绍可以看出决策树法具有许多优点：条理清晰，程序严谨，定量、<a href="http://wiki.mbalib.com/wiki/定性分析">定性分析</a>相结合，方法简单，易于掌握，应用性强，适用范围广等。</p></ul><h5 id="4贝叶斯分类器">4、贝叶斯分类器</h5><ul><li><p>贝叶斯分类器算法特点与介绍：</p><p>贝叶斯分类器是各种分类器中分类错误概率最小或者在预先给定代价的情况下平均风险最小的分类器。它的设计方法是一种最基本的统计<a href="https://baike.baidu.com/item/分类方法/9508629">分类方法</a>。其分类原理是通过某对象的<a href="https://baike.baidu.com/item/先验概率/6106649">先验概率</a>，利用<a href="https://baike.baidu.com/item/贝叶斯公式/9683982">贝叶斯公式</a>计算出其<a href="https://baike.baidu.com/item/后验概率">后验概率</a>，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。</p><p>https://blog.csdn.net/yangjingjing9/article/details/79986371</p><li><p>贝叶斯分类器使用</p><p>https://www.cnblogs.com/yechanglv/p/6947283.html</p><ul><li><p>python</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
</pre><td class="rouge-code"><pre>from numpy import *
    
class NaiveBayesClassifier(object):
        
    def __init__(self):
        self.dataMat = list()
        self.labelMat = list()
        self.pLabel1 = 0
        self.p0Vec = list()
        self.p1Vec = list()
    
    def loadDataSet(self,filename):
        fr = open(filename)
        for line in fr.readlines():
            lineArr = line.strip().split()
            dataLine = list()
            for i in lineArr:
                dataLine.append(float(i))
            label = dataLine.pop() # pop the last column referring to  label
            self.dataMat.append(dataLine)
            self.labelMat.append(int(label))
    
    
    def train(self):
        dataNum = len(self.dataMat)
        featureNum = len(self.dataMat[0])
        self.pLabel1 = sum(self.labelMat)/float(dataNum)
        p0Num = zeros(featureNum)
        p1Num = zeros(featureNum)
        p0Denom = 1.0
        p1Denom = 1.0
        for i in range(dataNum):
            if self.labelMat[i] == 1:
                p1Num += self.dataMat[i]
                p1Denom += sum(self.dataMat[i])
            else:
                p0Num += self.dataMat[i]
                p0Denom += sum(self.dataMat[i])
        self.p0Vec = p0Num/p0Denom
        self.p1Vec = p1Num/p1Denom
    
    def classify(self, data):
        p1 = reduce(lambda x, y: x * y, data * self.p1Vec) * self.pLabel1
        p0 = reduce(lambda x, y: x * y, data * self.p0Vec) * (1.0 - self.pLabel1)
        if p1 &gt; p0:
            return 1
        else: 
            return 0
    
    def test(self):
        self.loadDataSet('testNB.txt')
        self.train()
        print(self.classify([1, 2]))
    
if __name__ == '__main__':
    NB =  NaiveBayesClassifier()
    NB.test()
</pre></table></code></div></div><li><p>matlab</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>trainData = [0 1; -1 0; 2 2; 3 3; -2 -1;-4.5 -4; 2 -1; -1 -3];
group = [1 1 -1 -1 1 1 -1 -1]';
model = fitcnb(trainData, group)
testData = [5 2;3 1;-4 -3];
predict(model, testData)
</pre></table></code></div></div></ul></ul><h4 id="聚类模型整理模糊分类">聚类模型整理（模糊分类）</h4><h5 id="1k-means">1、k-means</h5><ul><li><p>k-means算法特点与介绍</p><p>算法最大的特点是简单，好理解，运算速度快，但是<strong>只能应用于连续型</strong>的数据，并且一定要在聚类前需要手工指定要分成几类</p><li><p>k-means原理：</p><ol><li>先从我们的数据库随机挑个随机点当“种子点”。<li>对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。<li>然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&lt;=0，此时的点就是下一个“种子点”。<li>重复第（2）和第（3）步直到所有的K个种子点都被选出来。<li>进行K-Means算法。</ol><li><p>k-means应用场景</p><p>k-means算法是聚类中较为常见的一种算法，通过比较类间距离与类外距离进行聚类，尽可能的缩小类间距离，增加类外距离。</p><p>一般应用于可以用距离来衡量两个物体相似度的场景，比如对于现有的客户分群……</p><li><p>k-means使用方法</p><ul><li><p>python： sklearn库：</p><p>python 中kmeans中各个参数的意义：https://www.cnblogs.com/wuchuanying/p/6218486.html</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
     
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="n">rcParams</span>
<span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span><span class="mi">8</span>
     
<span class="n">clustering_data_1</span> <span class="o">=</span> <span class="n">pandas</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'clustering_data_1.csv'</span><span class="p">)</span>   <span class="c1">#加载csv格式的数据
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">clustering_data_1</span><span class="p">[</span><span class="s">'X'</span><span class="p">],</span> <span class="n">clustering_data_1</span><span class="p">[</span><span class="s">'Y'</span><span class="p">])</span>    <span class="c1">#绘制原始数据
</span>     
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>     <span class="c1">#创建一个K-均值聚类对象
</span><span class="n">kmeans</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">clustering_data_1</span><span class="p">)</span>       <span class="c1">#拟合算法
</span><span class="n">cluster_assignment</span> <span class="o">=</span> <span class="n">cluster_assignment</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span> <span class="c1">#聚类分配，重新设置类别颜色
</span><span class="n">cluster_assignment</span><span class="p">[</span><span class="n">cluster_assignment</span> <span class="o">==</span> <span class="s">'0'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'b'</span>
<span class="n">cluster_assignment</span><span class="p">[</span><span class="n">cluster_assignment</span> <span class="o">==</span> <span class="s">'1'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'g'</span>
<span class="n">cluster_assignment</span><span class="p">[</span><span class="n">cluster_assignment</span> <span class="o">==</span> <span class="s">'2'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'r'</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">clustering_data_1</span><span class="p">[</span><span class="s">'X'</span><span class="p">],</span> <span class="n">clustering_data_1</span><span class="p">[</span><span class="s">'Y'</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">cluster_assignment</span><span class="p">)</span> <span class="c1">#绘制聚类结果
</span></pre></table></code></div></div><li><p>matlab：</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre><td class="rouge-code"><pre>使用方法：
      Idx=Kmeans(X,K)
      [Idx,C]=Kmeans(X,K) 
      [Idx,C,sumD]=Kmeans(X,K) 
      [Idx,C,sumD,D]=Kmeans(X,K) 
      […]=Kmeans(…,’Param1’,Val1,’Param2’,Val2,…)
各输入输出参数介绍：
       X :N*P的数据矩阵
       K: 表示将X划分为几类，为整数
       Idx :N*1的向量，存储的是每个点的聚类标号
       C: K*P的矩阵，存储的是K个聚类质心位置
      sumD 1*K的和向量，存储的是类间所有点与该类质心点距离之和
      D N*K的矩阵，存储的是每个点与所有质心的距离
      […]=Kmeans(…,'Param1',Val1,'Param2',Val2,…)
      这其中的参数Param1、Param2等，主要可以设置为如下：
      1. ‘Distance’(距离测度)
        ‘sqEuclidean’ 欧式距离（默认时，采用此距离方式）
        ‘cityblock’ 绝度误差和，又称：L1
        ‘cosine’ 针对向量
        ‘correlation’  针对有时序关系的值
        ‘Hamming’ 只针对二进制数据
      2. ‘Start’（初始质心位置选择方法）
        ‘sample’ 从X中随机选取K个质心点
        ‘uniform’ 根据X的分布范围均匀的随机生成K个质心
        ‘cluster’ 初始聚类阶段随机选择10%的X的子样本（此方法初始使用’sample’方法）
         matrix 提供一K*P的矩阵，作为初始质心位置集合
      3. ‘Replicates’（聚类重复次数）  整数
                 这个博客很好，忘记了就参考这个！
</pre></table></code></div></div></ul></ul><h5 id="2dbscan算法">2、DBSCAN算法</h5><ul><li><p>DBSCAN算法特点与介绍</p><p>DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一个比较有代表性的基于<a href="https://baike.baidu.com/item/密度/718381">密度</a>的<a href="https://baike.baidu.com/item/聚类算法/1252197">聚类算法</a>。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。</p><ul><li><p>优点</p><ol><li><p>与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。</p><li><p>与K-means方法相比，DBSCAN可以发现任意形状的簇类。</p><li><p>同时，DBSCAN能够识别出噪声点。</p><li><p>DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。</p></ol><li><p>缺点</p><ol><li><p>DBScan不能很好反映高维数据。</p><li>DBScan不能很好反映数据集以变化的密度。<li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差。</ol></ul><li><p>DBSCAN算法描述</p><p>（1）检测数据库中尚未检查过的对象<strong>p</strong>，如果<strong>p</strong>未被处理(归为某个簇或者标记为噪声)，则检查其邻域，若包含的对象数不小于minPts，建立新簇<strong>C</strong>，将其中的所有点加入候选集<strong>N</strong>；</p><p>（2）对候选集<strong>N</strong> 中所有尚未被处理的对象<strong>q</strong>，检查其邻域，若至少包含minPts个对象，则将这些对象加入N；如果<strong>q</strong> 未归入任何一个簇，则将<strong>q</strong> 加入<strong>C</strong>；</p><p>（3）重复步骤2)，继续检查<strong>N</strong> 中未处理的对象，当前候选集<strong>N为空</strong>；</p><p>（4）重复步骤1)~3)，直到所有对象都归入了某个簇或标记为噪声。</p><li><p>DBSCAN算法的应用</p><ul><li><p>python</p><p>https://www.jianshu.com/p/b004861105f4</p><p>class sklearn.cluster.DBSCAN(<em>eps=0.5</em>, <em>min_samples=5</em>, <em>metric=’euclidean’</em>, <em>algorithm=’auto’</em>, <em>leaf_size=30</em>, <em>p=None</em>, <em>n_jobs=1</em>)</p><div class="table-wrapper"><table><thead><tr><th>参数<th>说明<tbody><tr><td>eps<td>float，可选<tr><td>min_samples<td>int，可选<tr><td>metric<td>string，用于计算特征向量之间的距离<tr><td>algorithm<td>{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}，可选<tr><td>leaf_size<td>传递给球树，影响速度、内存，根据情况自己选择<tr><td>p<td>明氏距离的幂次，用于计算距离<tr><td>n_jobs<td>CPU并行数</table></div><div class="table-wrapper"><table><thead><tr><th>方法<th>说明<tbody><tr><td>fit(X[, y, sample_weight])<td>从特征矩阵进行聚类<tr><td>fit_predict(X[, y, sample_weight])<td>实行聚类并返回标签(n_samples, n_features)<tr><td>get_params([deep])<td>取得参数<tr><td>set_params(**params)<td>设置参数</table></div><div class="table-wrapper"><table><thead><tr><th>属性<th>类型<th>大小<th>说明<tbody><tr><td>core_sample_indices_<td>array<td>[n_core_samples]<td>核样本的目录<tr><td>components_<td>array<td>[n_core_samples, n_features]<td>训练样本的核样本<tr><td>labels_<td>array<td>[n_samples]<td>聚类标签。噪声样本标签为-1</table></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>import sklearn.cluster as skc
db=skc.DBSCAN(eps=0.01,min_samples=20).fit(X)
labels = db.labels_
</pre></table></code></div></div><li><p>matlab</p><p>https://blog.csdn.net/zhouxianen1987/article/details/68946169</p></ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/'>数学建模</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/" class="post-tag no-text-decoration" >数学建模</a> <a href="/tags/%E8%81%9A%E7%B1%BB/" class="post-tag no-text-decoration" >聚类</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=分类与聚类模型整理 - Tour&url=https://taolics.github.io/posts/%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=分类与聚类模型整理 - Tour&u=https://taolics.github.io/posts/%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=分类与聚类模型整理 - Tour&url=https://taolics.github.io/posts/%E5%88%86%E7%B1%BB%E4%B8%8E%E8%81%9A%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%95%B4%E7%90%86/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/%E4%BB%8E%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%88%B0%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83-PCDet/">从装系统到配置深度学习环境（PCDet）</a><li><a href="/posts/%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">分类器性能评价指标</a><li><a href="/posts/%E4%BB%8E%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98%E8%AE%A1%E7%AE%97%E6%AC%A1%E5%BA%8F%E9%97%AE%E9%A2%98%E5%BC%95%E5%87%BA%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">从矩阵连乘计算次序问题引出动态规划</a><li><a href="/posts/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%E4%B8%8E%E5%88%A4%E6%96%AD%E6%9C%89%E5%90%91%E5%9B%BE%E6%98%AF%E5%90%A6%E6%9C%89%E5%9B%9E%E8%B7%AF/">拓扑排序与判断有向图是否有回路</a><li><a href="/posts/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E4%B8%8E%E5%88%A4%E6%96%AD%E6%97%A0%E5%90%91%E5%9B%BE%E6%98%AF%E5%90%A6%E6%9C%89%E5%9B%9E%E8%B7%AF-%E5%B9%B6%E6%9F%A5%E9%9B%86/">最小生成树与判断无向图是否有回路（并查集）</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/avl%E6%A0%91/">AVL树</a> <a class="post-tag" href="/tags/bfprt%E7%AE%97%E6%B3%95/">BFPRT算法</a> <a class="post-tag" href="/tags/bfs/">BFS</a> <a class="post-tag" href="/tags/bst/">BST</a> <a class="post-tag" href="/tags/dfs/">DFS</a> <a class="post-tag" href="/tags/dijkstra%E7%AE%97%E6%B3%95/">Dijkstra算法</a> <a class="post-tag" href="/tags/google/">Google</a> <a class="post-tag" href="/tags/kmp%E7%AE%97%E6%B3%95/">KMP算法</a> <a class="post-tag" href="/tags/php/">php</a> <a class="post-tag" href="/tags/rk%E7%AE%97%E6%B3%95/">RK算法</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/%E4%BB%8E%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%88%B0%E9%85%8D%E7%BD%AE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83-PCDet/"><div class="card-body"> <span class="timeago small" > May 30 <i class="unloaded">2021-05-30T23:32:05+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>从装系统到配置深度学习环境（PCDet）</h3><div class="text-muted small"><p> 最终环境配置如下 1 2 3 4 5 6 7 8 9 显卡：NVIDIA RTX 2070; 操作系统：Ubuntu18.04; pytorch 1.8.0; Python 3.7.10; cuda 11.0; cudnn 8.0.5; cmake 3.16.3; g++ 7.5 ------以下是PCdet基本环境----- spconv 1.2.1 pcde...</p></div></div></a></div><div class="card"> <a href="/posts/%E5%88%86%E7%B1%BB%E5%99%A8%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"><div class="card-body"> <span class="timeago small" > Mar 22 <i class="unloaded">2021-03-22T12:53:05+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>分类器性能评价指标</h3><div class="text-muted small"><p> 通用指标 1. 混淆矩阵confusion matrix TP(truth positive)：本身（真实情况）是阳性，检测（决策）出来也是阳性 FP(false positive)：本身是阴性，检测出来是阳性 TN(truth negative)：本身是阴性，检测出来也是阴性 FN(false negative...</p></div></div></a></div><div class="card"> <a href="/posts/%E4%BB%8E%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%AE%9E%E9%AA%8C%E5%88%B0%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83/"><div class="card-body"> <span class="timeago small" > Mar 7 <i class="unloaded">2021-03-07T21:35:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>从伯努利实验到概率分布</h3><div class="text-muted small"><p> 0. 基础知识 什么是分布distribution？全称概率分布，也叫分布律、概率函数，用来表述随机变量取值的概率规律，也就是数据在统计图中的形状，也就是随机变量X取某个值的概率P是多少 什么是分布函数？由分布的定义我们可以知道，概率P是随机变量X的函数，因此我们定义 分布函数F(x)，是指随机变量X小于x时的概率之和，即F(x) = P(X&lt;=x) 由于数据有离散型和...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/%E5%9B%9E%E6%BA%AF%E6%B3%95/" class="btn btn-outline-primary"><p>回溯法</p></a> <a href="/posts/%E7%BD%91%E9%A1%B5%E8%AE%BE%E8%AE%A1%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0%E6%9C%8D%E5%8A%A1%E5%99%A8/" class="btn btn-outline-primary"><p>网页设计并部署到服务器</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://www.cnblogs.com/icodes8238/">taolics</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/avl%E6%A0%91/">AVL树</a> <a class="post-tag" href="/tags/bfprt%E7%AE%97%E6%B3%95/">BFPRT算法</a> <a class="post-tag" href="/tags/bfs/">BFS</a> <a class="post-tag" href="/tags/bst/">BST</a> <a class="post-tag" href="/tags/dfs/">DFS</a> <a class="post-tag" href="/tags/dijkstra%E7%AE%97%E6%B3%95/">Dijkstra算法</a> <a class="post-tag" href="/tags/google/">Google</a> <a class="post-tag" href="/tags/kmp%E7%AE%97%E6%B3%95/">KMP算法</a> <a class="post-tag" href="/tags/php/">php</a> <a class="post-tag" href="/tags/rk%E7%AE%97%E6%B3%95/">RK算法</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script> <script> $(function() { let initTheme = "default"; if ($("html[mode=dark]").length > 0 || ($("html[mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://taolics.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
