---
title: 分类与聚类模型整理
author: Tour
date: 2020-09-04 14:07:00 +0800
categories: [数学建模]
tags: [数学建模, 聚类]
math: true
mermaid: true
---

#### 分类模型整理

##### 1、SVM

* SVM算法特点与介绍

  * 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以 SVM 不太容易产生 overfitting。

    SVM 训练出来的模型完全依赖于支持向量，即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。
    
    一个 SVM 如果训练得出的支持向量个数比较少，那么SVM 训练出的模型比较容易被泛化。
    
  * 优点：
    SVM在中小量样本规模的时候容易得到数据和特征之间的非线性关系，可以避免使用神经网络结构选择和局部极小值问题，可解释性强，可以解决高维问题。

  * 缺点：
    SVM对缺失数据敏感，对非线性问题没有通用的解决方案，核函数的正确选择不容易，计算复杂度高，主流的算法可以达到$O(n_2)$的复杂度，这对大规模的数据是吃不消的。

* SVM原理

  支持向量机（Support Vector Machine, SVM）的基本模型是在特征空间上找到最佳的分离超平面使得训练集上正负样本间隔最大。SVM是用来解决二分类问题的有监督学习算法，在引入了核方法之后SVM也可以用来解决非线性问题。
  一般SVM有下面三种：

  - 硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，可通过硬间隔最大化学得一个线性可分支持向量机。
  - 软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化学得一个线性支持向量机。
  - 非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化学得一个非线性支持向量机。

  1、周志华. 机器学习 [D]. 清华大学出版社，2016.

  2、https://blog.csdn.net/liugan528/article/details/79448379

  3、https://zhuanlan.zhihu.com/p/31886934

* SVM应用场景

  SVM在小样本训练集上能够得到比其它算法好很多的结果。支持向量机之所以成为目前最常用，效果最好的分类器之一，在于其优秀的泛化能力，这是是因为其本身的优化目标是结构化风险最小，而不是经验风险最小。

* SVM使用方法

  * python

    https://blog.csdn.net/u012679707/article/details/80511968

    重要参数详解：https://blog.csdn.net/github_39261590/article/details/75009069
  
    训练svm分类器
    kernel='linear'时，为线性核，C越大分类效果越好，但有可能会过拟合（defaul C=1）
    kernel='rbf'时（default），为高斯核，gamma值越小，分类界面越连续；gamma值越大，分类界面越“散”，分类效果越好，但有可能会过拟合。
  decision_function_shape='ovr'时，为one v rest（一对多），即一个类别与其他类别进行划分，
    decision_function_shape='ovo'时，为one v one（一对一），即将类别两两之间进行划分，用二分#类的方法模拟多分类的结果。
  
    ```
    from sklearn import svm
    classifier=svm.SVC(C=2,kernel='rbf',gamma=10,decision_function_shape='ovo') 
  classifier.fit(train_data,train_label.ravel()) #ravel函数在降维时默认是行序优先
    ```

  * matlab中使用
  
    ```
    SVMStruct = svmtrain(TrainData,Group,'Showplot',true);       % train
    Group = svmclassify(SVMStruct,TestData,'Showplot',true);     % test
    ```

##### 2、逻辑回归算法

* 逻辑回归算法特点与介绍

  逻辑回归（Logistic Regression**）**是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。 如给的一封邮件，判断是不是垃圾邮件。逻辑回归一般是提供样本和已知模型求回归参数。

  一般是解决二分类问题，一般判断某个样本事件所属分类的概率。

* 逻辑回归使用

  * python

    重要参数解释：https://blog.csdn.net/Monk_donot_know/article/details/90720366?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf
  
    ```
    from sklearn.linear_model import LogisticRegression
    
    #训练模型
    lr=LogisticRegression(C=1e5)
    lr.fit(X,Y)
    
    #预测
    Z=lr.predict(np.c_[xx.ravel(),yy.ravel()])
    Z=Z.reshape(xx.shape)
  
    ```

  * matlab
  
    ```
    a =glmfit(x,y,'binomial', 'link', 'logit');  //用逻辑回归来计算系数矩阵
  logitFit = glmval(a,x, 'logit'); //用逻辑回归的结果预测测试集的结果
    ```
  
    

##### 3、决策树算法

* 决策树算法特点与介绍

  决策树(Decision Tree）是在已知各种情况发生概率的[基础](https://baike.baidu.com/item/基础/32794)上，通过构成决策树来求取净现值的[期望](https://baike.baidu.com/item/期望/35704)值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。Entropy = 系统的凌乱程度，使用算法[ID3](https://baike.baidu.com/item/ID3), [C4.5](https://baike.baidu.com/item/C4.5)和C5.0生成树算法使用熵。这一度量是基于信息学理论中熵的概念。

  决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。

* 决策树的使用方法：

  * python

    https://www.cnblogs.com/juanjiang/p/11003369.html

    https://blog.csdn.net/xiaoyw71/article/details/80110658?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.add_param_isCf

    重要参数详解：https://www.cnblogs.com/juanjiang/p/11003369.html

    ```
    from sklearn import tree
    '''使用信息熵作为划分标准，对决策树进行训练 '''  
    clf = tree.DecisionTreeClassifier(criterion='entropy')  
    print(clf)     
    clf.fit(x_train, y_train)  
    answer = clf.predict(x_train) 
    ```

    

  * matlab

    ```
    
    ctree = ClassificationTree.fit(P_train,T_train);
    %%得到最小的叶子尺寸，再进行决策树生成
    OptimalTree = fitctree(X,Y,'minleaf',40);
    %%查看决策树
    view(ctree);
    view(ctree,'mode','graph');
    %% 剪枝
    [~,~,~,bestlevel] = cvLoss(ctree,'subtrees','all','treesize','min')
    cptree = prune(ctree,'Level',bestlevel);
    view(cptree,'mode','graph')
    ```

* 决策树应用场景：

  1、应用决策树决策方法必须具备以下条件：

  （1）具有决策者期望达到的明确目标

  （2）存在决策者可以选择的两个以上的可行的备选方案

  （3）存在决策者无法控制的两个以上不确定因素

  （4）不同方案在不同因素下的收益或损失可以计算出来

  （5）决策者可以估计不确定因素发生的概率

  2、从以上介绍可以看出决策树法具有许多优点：条理清晰，程序严谨，定量、[定性分析](http://wiki.mbalib.com/wiki/定性分析)相结合，方法简单，易于掌握，应用性强，适用范围广等。

##### 4、贝叶斯分类器

* 贝叶斯分类器算法特点与介绍：

  贝叶斯分类器是各种分类器中分类错误概率最小或者在预先给定代价的情况下平均风险最小的分类器。它的设计方法是一种最基本的统计[分类方法](https://baike.baidu.com/item/分类方法/9508629)。其分类原理是通过某对象的[先验概率](https://baike.baidu.com/item/先验概率/6106649)，利用[贝叶斯公式](https://baike.baidu.com/item/贝叶斯公式/9683982)计算出其[后验概率](https://baike.baidu.com/item/后验概率)，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。

  https://blog.csdn.net/yangjingjing9/article/details/79986371

* 贝叶斯分类器使用

  https://www.cnblogs.com/yechanglv/p/6947283.html

  * python

    ```
    from numpy import *
    
    class NaiveBayesClassifier(object):
        
        def __init__(self):
            self.dataMat = list()
            self.labelMat = list()
            self.pLabel1 = 0
            self.p0Vec = list()
            self.p1Vec = list()
    
        def loadDataSet(self,filename):
            fr = open(filename)
            for line in fr.readlines():
                lineArr = line.strip().split()
                dataLine = list()
                for i in lineArr:
                    dataLine.append(float(i))
                label = dataLine.pop() # pop the last column referring to  label
                self.dataMat.append(dataLine)
                self.labelMat.append(int(label))
    
    
        def train(self):
            dataNum = len(self.dataMat)
            featureNum = len(self.dataMat[0])
            self.pLabel1 = sum(self.labelMat)/float(dataNum)
            p0Num = zeros(featureNum)
            p1Num = zeros(featureNum)
            p0Denom = 1.0
            p1Denom = 1.0
            for i in range(dataNum):
                if self.labelMat[i] == 1:
                    p1Num += self.dataMat[i]
                    p1Denom += sum(self.dataMat[i])
                else:
                    p0Num += self.dataMat[i]
                    p0Denom += sum(self.dataMat[i])
            self.p0Vec = p0Num/p0Denom
            self.p1Vec = p1Num/p1Denom
    
        def classify(self, data):
            p1 = reduce(lambda x, y: x * y, data * self.p1Vec) * self.pLabel1
            p0 = reduce(lambda x, y: x * y, data * self.p0Vec) * (1.0 - self.pLabel1)
            if p1 > p0:
                return 1
            else: 
                return 0
    
        def test(self):
            self.loadDataSet('testNB.txt')
            self.train()
            print(self.classify([1, 2]))
    
    if __name__ == '__main__':
        NB =  NaiveBayesClassifier()
        NB.test()
    ```

    

  * matlab

    ```
    trainData = [0 1; -1 0; 2 2; 3 3; -2 -1;-4.5 -4; 2 -1; -1 -3];
    group = [1 1 -1 -1 1 1 -1 -1]';
    model = fitcnb(trainData, group)
    testData = [5 2;3 1;-4 -3];
    predict(model, testData)
    ```

    

#### 聚类模型整理（模糊分类）

##### 1、k-means

* k-means算法特点与介绍

  算法最大的特点是简单，好理解，运算速度快，但是**只能应用于连续型**的数据，并且一定要在聚类前需要手工指定要分成几类

* k-means原理：

  1. 先从我们的数据库随机挑个随机点当“种子点”。
  2. 对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。
  3. 然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其<=0，此时的点就是下一个“种子点”。
  4. 重复第（2）和第（3）步直到所有的K个种子点都被选出来。
  5. 进行K-Means算法。

* k-means应用场景

  k-means算法是聚类中较为常见的一种算法，通过比较类间距离与类外距离进行聚类，尽可能的缩小类间距离，增加类外距离。

  一般应用于可以用距离来衡量两个物体相似度的场景，比如对于现有的客户分群......

* k-means使用方法

  * python： sklearn库：

    python 中kmeans中各个参数的意义：https://www.cnblogs.com/wuchuanying/p/6218486.html
  
    ```python
    import pandas
    from sklearn.cluster import KMeans
     
    import matplotlib.pyplot as plt
    from pylab import rcParams
    rcParams['figure.figsize'] = 10,8
     
    clustering_data_1 = pandas.read_csv('clustering_data_1.csv')   #加载csv格式的数据
    plt.scatter(clustering_data_1['X'], clustering_data_1['Y'])    #绘制原始数据
     
    kmeans = KMeans(n_clusters = 3)     #创建一个K-均值聚类对象
    kmeans.fit(clustering_data_1)       #拟合算法
    cluster_assignment = cluster_assignment.astype(str) #聚类分配，重新设置类别颜色
    cluster_assignment[cluster_assignment == '0'] = 'b'
    cluster_assignment[cluster_assignment == '1'] = 'g'
    cluster_assignment[cluster_assignment == '2'] = 'r'
  plt.scatter(clustering_data_1['X'], clustering_data_1['Y'], c=cluster_assignment) #绘制聚类结果
    ```

  * matlab：
  
    ```
    使用方法：
          Idx=Kmeans(X,K)
          [Idx,C]=Kmeans(X,K) 
          [Idx,C,sumD]=Kmeans(X,K) 
          [Idx,C,sumD,D]=Kmeans(X,K) 
          […]=Kmeans(…,’Param1’,Val1,’Param2’,Val2,…)
    各输入输出参数介绍：
           X :N*P的数据矩阵
           K: 表示将X划分为几类，为整数
           Idx :N*1的向量，存储的是每个点的聚类标号
           C: K*P的矩阵，存储的是K个聚类质心位置
          sumD 1*K的和向量，存储的是类间所有点与该类质心点距离之和
          D N*K的矩阵，存储的是每个点与所有质心的距离
          […]=Kmeans(…,'Param1',Val1,'Param2',Val2,…)
          这其中的参数Param1、Param2等，主要可以设置为如下：
          1. ‘Distance’(距离测度)
            ‘sqEuclidean’ 欧式距离（默认时，采用此距离方式）
            ‘cityblock’ 绝度误差和，又称：L1
            ‘cosine’ 针对向量
            ‘correlation’  针对有时序关系的值
            ‘Hamming’ 只针对二进制数据
          2. ‘Start’（初始质心位置选择方法）
            ‘sample’ 从X中随机选取K个质心点
            ‘uniform’ 根据X的分布范围均匀的随机生成K个质心
            ‘cluster’ 初始聚类阶段随机选择10%的X的子样本（此方法初始使用’sample’方法）
             matrix 提供一K*P的矩阵，作为初始质心位置集合
          3. ‘Replicates’（聚类重复次数）  整数
                     这个博客很好，忘记了就参考这个！
    ```
  
    

##### 2、DBSCAN算法

* DBSCAN算法特点与介绍

  DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一个比较有代表性的基于[密度](https://baike.baidu.com/item/密度/718381)的[聚类算法](https://baike.baidu.com/item/聚类算法/1252197)。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。

  * 优点

    1. 与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。

    2. 与K-means方法相比，DBSCAN可以发现任意形状的簇类。

    3. 同时，DBSCAN能够识别出噪声点。

    4. DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。

  * 缺点

    1. DBScan不能很好反映高维数据。

    2. DBScan不能很好反映数据集以变化的密度。
    3. 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差。

* DBSCAN算法描述

  （1）检测数据库中尚未检查过的对象**p**，如果**p**未被处理(归为某个簇或者标记为噪声)，则检查其邻域，若包含的对象数不小于minPts，建立新簇**C**，将其中的所有点加入候选集**N**；

  （2）对候选集**N** 中所有尚未被处理的对象**q**，检查其邻域，若至少包含minPts个对象，则将这些对象加入N；如果**q** 未归入任何一个簇，则将**q** 加入**C**；

  （3）重复步骤2)，继续检查**N** 中未处理的对象，当前候选集**N为空**；

  （4）重复步骤1)~3)，直到所有对象都归入了某个簇或标记为噪声。

  

* DBSCAN算法的应用

  * python

    https://www.jianshu.com/p/b004861105f4

    class sklearn.cluster.DBSCAN(*eps=0.5*, *min_samples=5*, *metric='euclidean'*, *algorithm='auto'*, *leaf_size=30*, *p=None*, *n_jobs=1*)

    | 参数        | 说明                                            |
    | ----------- | ----------------------------------------------- |
    | eps         | float，可选                                     |
    | min_samples | int，可选                                       |
    | metric      | string，用于计算特征向量之间的距离              |
    | algorithm   | {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}，可选 |
    | leaf_size   | 传递给球树，影响速度、内存，根据情况自己选择    |
    | p           | 明氏距离的幂次，用于计算距离                    |
    | n_jobs      | CPU并行数                                       |

    | 方法                               | 说明                                      |
    | ---------------------------------- | ----------------------------------------- |
    | fit(X[, y, sample_weight])         | 从特征矩阵进行聚类                        |
    | fit_predict(X[, y, sample_weight]) | 实行聚类并返回标签(n_samples, n_features) |
    | get_params([deep])                 | 取得参数                                  |
    | set_params(**params)               | 设置参数                                  |

    | 属性                 | 类型  | 大小                         | 说明                       |
    | -------------------- | ----- | ---------------------------- | -------------------------- |
    | core_sample_indices_ | array | [n_core_samples]             | 核样本的目录               |
    | components_          | array | [n_core_samples, n_features] | 训练样本的核样本           |
    | labels_              | array | [n_samples]                  | 聚类标签。噪声样本标签为-1 |

    ```
    import sklearn.cluster as skc
    db=skc.DBSCAN(eps=0.01,min_samples=20).fit(X)
    labels = db.labels_
    ```

    

  * matlab

    https://blog.csdn.net/zhouxianen1987/article/details/68946169